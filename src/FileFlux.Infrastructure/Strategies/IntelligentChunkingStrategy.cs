using FileFlux;
using FileFlux.Domain;
using System.Text.RegularExpressions;

namespace FileFlux.Infrastructure.Strategies;

/// <summary>
/// ì§€ëŠ¥í˜• ì²­í‚¹ ì „ëµ - RAG ì‹œìŠ¤í…œì— ìµœì í™”ëœ ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ë¶„í• 
/// </summary>
public partial class IntelligentChunkingStrategy : IChunkingStrategy
{
    private static readonly Regex SentenceEndRegex = MyRegex();
    private static readonly Regex ParagraphRegex = new(@"\n\s*\n+", RegexOptions.Compiled);
    private static readonly Regex HeaderRegex = new(@"^#{1,6}\s+.+$|^.+\n[=\-]+\s*$", RegexOptions.Compiled | RegexOptions.Multiline);
    private static readonly Regex ListItemRegex = new(@"^\s*[-*+]\s+|^\s*\d+\.\s+", RegexOptions.Compiled | RegexOptions.Multiline);
    private static readonly Regex ImportantKeywordRegex = new(@"\b(ì¤‘ìš”|í•µì‹¬|ìš”ì•½|ê²°ë¡ |ì°¸ê³ |ì£¼ì˜|ê²½ê³ |important|key|summary|conclusion|note|warning|attention)\b", RegexOptions.Compiled | RegexOptions.IgnoreCase);

    // í…Œì´ë¸” ê°ì§€ë¥¼ ìœ„í•œ ì •ê·œì‹ ì¶”ê°€
    private static readonly Regex TableRegex = new(@"^\s*\|[^|]+\|[^|]+\|", RegexOptions.Compiled | RegexOptions.Multiline);
    private static readonly Regex TableSeparatorRegex = new(@"^\s*\|[\s\-\|]+\|", RegexOptions.Compiled | RegexOptions.Multiline);
    private static readonly Regex MarkdownSectionRegex = new(@"^#{1,6}\s+.*$", RegexOptions.Compiled | RegexOptions.Multiline);
    
    // Phase 10: Context Preservation ê°•í™”ë¥¼ ìœ„í•œ ì ì‘í˜• ì˜¤ë²„ë© ë§¤ë‹ˆì €
    private static readonly AdaptiveOverlapManager _overlapManager = new();
    
    // Phase 10: Boundary Quality ì¼ê´€ì„± ê°œì„ ì„ ìœ„í•œ ê²½ê³„ í’ˆì§ˆ ë§¤ë‹ˆì €
    private static readonly BoundaryQualityManager _boundaryQualityManager = new();
    
    // Phase 11: Vector Search Optimization components
    private static readonly VectorSearchOptimizer _vectorSearchOptimizer = new();
    private static readonly SearchMetadataEnricher _metadataEnricher = new();
    private static readonly HybridSearchPreprocessor _hybridPreprocessor = new();
    private static readonly SearchQualityEvaluator _qualityEvaluator = new();
    
    // Phase 12: Graph Search Optimization components
    private static readonly EntityExtractionSystem _entityExtractor = new();
    private static readonly GraphStructureGenerator _graphGenerator = new();
    private static readonly OntologyMapper _ontologyMapper = new();
    private static readonly GraphQualityAssurance _graphQualityAssurance = new();

    public string StrategyName => ChunkingStrategies.Intelligent;

    public IEnumerable<string> SupportedOptions => new[]
    {
        "ContextWindowSize",      // ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° í¬ê¸°
        "SemanticCoherence",      // ì˜ë¯¸ì  ì‘ì§‘ì„± ê°€ì¤‘ì¹˜
        "ImportanceWeighting",    // ì¤‘ìš”ë„ ê°€ì¤‘ì¹˜
        "StructuralAwareness",    // êµ¬ì¡°ì  ì¸ì‹ ì—¬ë¶€
        "AdaptiveOverlap",        // ì ì‘í˜• ê²¹ì¹¨ í¬ê¸°
        "QualityThreshold"        // í’ˆì§ˆ ì„ê³„ê°’
    };
    private static readonly char[] separator = new[] { ' ', '\n', '\r', '\t' };

    public async Task<IEnumerable<DocumentChunk>> ChunkAsync(
        DocumentContent content,
        ChunkingOptions options,
        CancellationToken cancellationToken = default)
    {
        ArgumentNullException.ThrowIfNull(content);

        if (string.IsNullOrWhiteSpace(content.Text))
            return Enumerable.Empty<DocumentChunk>();

        var chunks = new List<DocumentChunk>();
        var text = content.Text;

        // ì „ì²´ ë¬¸ì„œì˜ LLM ìµœì í™” ì»¨í…ìŠ¤íŠ¸ë¥¼ ë¨¼ì € ë¶„ì„
        var globalTechKeywords = DetectTechnicalKeywords(text);
        var globalDocumentDomain = DetectDocumentDomain(text, globalTechKeywords);

        // ì „ëµ ì˜µì…˜ ê°€ì ¸ì˜¤ê¸°
        var contextWindowSize = GetStrategyOption(options, "ContextWindowSize", options.MaxChunkSize);
        var semanticCoherence = GetStrategyOption(options, "SemanticCoherence", 0.7);
        var importanceWeighting = GetStrategyOption(options, "ImportanceWeighting", 0.8);
        var structuralAwareness = GetStrategyOption(options, "StructuralAwareness", true);
        var adaptiveOverlap = GetStrategyOption(options, "AdaptiveOverlap", true);
        var qualityThreshold = GetStrategyOption(options, "QualityThreshold", 0.6);

        // 1ë‹¨ê³„: ë¬¸ì„œ êµ¬ì¡° ë¶„ì„
        var documentStructure = AnalyzeDocumentStructure(text);

        // 2ë‹¨ê³„: ì˜ë¯¸ì  ë‹¨ìœ„ ì¶”ì¶œ
        var semanticUnits = ExtractSemanticUnits(text, documentStructure);

        // í…Œì´ë¸” ê°ì§€ ì‹œ ì²­í‚¹ í¬ê¸° ë™ì  ì¡°ì •
        var effectiveWindowSize = ContainsAnyTable(semanticUnits) ? contextWindowSize * 2 : contextWindowSize;

        // 3ë‹¨ê³„: ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ì²­í‚¹ (Phase 10: ì ì‘í˜• ì˜¤ë²„ë© ì ìš©)
        var contextualChunks = CreateContextualChunks(
            semanticUnits,
            effectiveWindowSize,
            semanticCoherence,
            options, // ì „ì²´ ì˜µì…˜ ì „ë‹¬í•˜ì—¬ ì ì‘í˜• ì˜¤ë²„ë© ê³„ì‚°
            adaptiveOverlap);

        // 4ë‹¨ê³„: í’ˆì§ˆ í‰ê°€ ë° ìµœì í™” - í…Œì´ë¸” ìˆì„ ë•Œ ë™ì  í¬ê¸° ì ìš©
        var effectiveMaxSize = ContainsAnyTable(semanticUnits) ? options.MaxChunkSize * 2 : options.MaxChunkSize;
        var optimizedChunks = OptimizeChunks(contextualChunks, qualityThreshold, effectiveMaxSize);

        // 5ë‹¨ê³„: ìµœì¢… ì²­í¬ ìƒì„±
        var chunkIndex = 0;
        var globalPosition = 0;

        foreach (var chunkContent in optimizedChunks)
        {
            cancellationToken.ThrowIfCancellationRequested();

            var chunk = CreateIntelligentChunk(
                chunkContent,
                content.Metadata,
                chunkIndex++,
                globalPosition,
                options,
                importanceWeighting,
                globalTechKeywords,
                globalDocumentDomain);

            chunks.Add(chunk);
            globalPosition += chunkContent.Length;
        }

        return await Task.FromResult(chunks);
    }

    public int EstimateChunkCount(DocumentContent content, ChunkingOptions options)
    {
        if (content == null || string.IsNullOrWhiteSpace(content.Text))
            return 0;

        var contextWindowSize = GetStrategyOption(options, "ContextWindowSize", options.MaxChunkSize);
        var avgChunkSize = (int)(contextWindowSize * 0.8); // í‰ê· ì ìœ¼ë¡œ 80% ì‚¬ìš©

        return (int)Math.Ceiling((double)content.Text.Length / avgChunkSize);
    }

    private static DocumentStructure AnalyzeDocumentStructure(string text)
    {
        var structure = new DocumentStructure();

        // í—¤ë” ì°¾ê¸°
        structure.Headers = HeaderRegex.Matches(text)
            .Cast<Match>()
            .Select(m => new StructuralElement
            {
                Content = m.Value.Trim(),
                Position = m.Index,
                Type = "Header",
                Importance = CalculateHeaderImportance(m.Value)
            })
            .ToList();

        // ë¦¬ìŠ¤íŠ¸ í•­ëª© ì°¾ê¸°
        structure.ListItems = ListItemRegex.Matches(text)
            .Cast<Match>()
            .Select(m => new StructuralElement
            {
                Content = ExtractListItem(text, m.Index),
                Position = m.Index,
                Type = "ListItem",
                Importance = 0.5
            })
            .ToList();

        // ë¬¸ë‹¨ ê²½ê³„ ì°¾ê¸°
        var paragraphs = ParagraphRegex.Split(text)
            .Where(p => !string.IsNullOrWhiteSpace(p))
            .Select((p, i) => new StructuralElement
            {
                Content = p.Trim(),
                Position = text.IndexOf(p.Trim()),
                Type = "Paragraph",
                Importance = CalculateParagraphImportance(p.Trim())
            })
            .ToList();

        structure.Paragraphs = paragraphs;

        return structure;
    }

    private static List<SemanticUnit> ExtractSemanticUnits(string text, DocumentStructure structure)
    {
        var units = new List<SemanticUnit>();
        var lines = text.Split('\n');
        var position = 0;

        for (int i = 0; i < lines.Length; i++)
        {
            var line = lines[i].TrimEnd();
            if (string.IsNullOrWhiteSpace(line))
            {
                position += lines[i].Length + 1; // +1 for newline
                continue;
            }

            // í…Œì´ë¸” í–‰ ê°ì§€ - í…Œì´ë¸” í–‰ë“¤ì„ í•˜ë‚˜ì˜ ë‹¨ìœ„ë¡œ ë¬¶ê¸°
            if (IsTableLine(line))
            {
                var tableUnit = ExtractTableUnit(lines, i, position, out var endIndex, out var nextPosition);
                if (tableUnit != null)
                {
                    units.Add(tableUnit);
                    i = endIndex;
                    position = nextPosition;
                    continue;
                }
            }

            // ì„¹ì…˜ í—¤ë” ê°ì§€
            var isHeader = MarkdownSectionRegex.IsMatch(line);

            // ì¼ë°˜ ë¼ì¸ ì²˜ë¦¬
            var unit = new SemanticUnit
            {
                Content = line,
                Position = position,
                SemanticWeight = CalculateSemanticWeight(line),
                ContextualRelevance = CalculateContextualRelevance(line, structure),
                Importance = isHeader ? 1.0 : CalculateImportance(line)
            };

            units.Add(unit);
            position += lines[i].Length + 1; // +1 for newline
        }

        return units;
    }

    /// <summary>
    /// í…Œì´ë¸” ë¼ì¸ ë˜ëŠ” í…Œì´ë¸” ë§ˆì»¤ì¸ì§€ í™•ì¸
    /// </summary>
    private static bool IsTableLine(string line)
    {
        // Markdown í…Œì´ë¸” ë¼ì¸: |ë¡œ ì‹œì‘í•˜ê±°ë‚˜ |ë¥¼ 2ê°œ ì´ìƒ í¬í•¨í•˜ëŠ” ë¼ì¸
        var trimmed = line.Trim();
        if (string.IsNullOrEmpty(trimmed)) return false;

        // TABLE_START ë§ˆì»¤ì¸ì§€ í™•ì¸
        if (trimmed.Contains("<!-- TABLE_START -->")) return true;

        // |ë¥¼ 2ê°œ ì´ìƒ í¬í•¨í•˜ëŠ” ë¼ì¸ì´ë©´ í…Œì´ë¸” ë¼ì¸ìœ¼ë¡œ íŒë‹¨
        return trimmed.Contains('|') && trimmed.Count(c => c == '|') >= 2;
    }

    /// <summary>
    /// í…Œì´ë¸” ë§ˆì»¤ë¥¼ í¬í•¨í•œ ì™„ì „í•œ í…Œì´ë¸” ë¸”ë¡ì„ í•˜ë‚˜ì˜ SemanticUnitìœ¼ë¡œ ì¶”ì¶œ
    /// TABLE_STARTë¶€í„° TABLE_ENDê¹Œì§€ì˜ ëª¨ë“  ë‚´ìš©ì„ í¬í•¨
    /// </summary>
    private static SemanticUnit? ExtractTableUnit(string[] lines, int startIndex, int startPosition, out int endIndex, out int nextPosition)
    {
        var tableLines = new List<string>();
        endIndex = startIndex;
        nextPosition = startPosition;
        // í˜„ì¬ ë¼ì¸ì´ TABLE_STARTì¸ì§€ í™•ì¸
        if (lines[startIndex].Contains("<!-- TABLE_START -->"))
        {
            tableLines.Add(lines[startIndex]);
            nextPosition += lines[startIndex].Length + 1;

            // TABLE_ENDê¹Œì§€ ëª¨ë“  ë¼ì¸ ìˆ˜ì§‘
            for (int i = startIndex + 1; i < lines.Length; i++)
            {
                var line = lines[i];
                tableLines.Add(line);
                endIndex = i;
                nextPosition += lines[i].Length + 1;

                if (line.Contains("<!-- TABLE_END -->"))
                {
                    break; // í…Œì´ë¸” ë¸”ë¡ ì™„ë£Œ
                }
            }

            // ì™„ì „í•œ í…Œì´ë¸” ë¸”ë¡ì´ ìˆ˜ì§‘ë˜ì—ˆìœ¼ë©´ SemanticUnit ìƒì„±
            if (tableLines.Count != 0 && tableLines.Last().Contains("<!-- TABLE_END -->"))
            {
                return new SemanticUnit
                {
                    Content = string.Join("\n", tableLines),
                    Position = startPosition,
                    SemanticWeight = 1.0, // í…Œì´ë¸”ì€ ë†’ì€ ê°€ì¤‘ì¹˜
                    ContextualRelevance = 1.0,
                    Importance = 0.9
                };
            }
        }
        else
        {
            // ê¸°ì¡´ ë¡œì§: ì—°ì†ëœ í…Œì´ë¸” ë¼ì¸ ìˆ˜ì§‘ (ë§ˆì»¤ê°€ ì—†ëŠ” ê²½ìš°)
            for (int i = startIndex; i < lines.Length; i++)
            {
                var line = lines[i].TrimEnd();

                if (IsTableLine(line))
                {
                    tableLines.Add(line);
                    endIndex = i;
                    nextPosition += lines[i].Length + 1;
                }
                else if (string.IsNullOrWhiteSpace(line) && tableLines.Count != 0)
                {
                    // í…Œì´ë¸” ë‚´ì˜ ë¹ˆ ì¤„ì€ í—ˆìš©í•˜ì§€ë§Œ í…Œì´ë¸”ì— í¬í•¨ì‹œí‚¤ì§€ ì•ŠìŒ
                    nextPosition += lines[i].Length + 1;
                }
                else if (tableLines.Count != 0) // í…Œì´ë¸” ë¼ì¸ì´ ìˆì—ˆëŠ”ë° í…Œì´ë¸”ì´ ì•„ë‹Œ ë¼ì¸ì„ ë§Œë‚˜ë©´ ì¢…ë£Œ
                {
                    break;
                }
                else // ì²« ë²ˆì§¸ ë¼ì¸ì´ í…Œì´ë¸”ì´ ì•„ë‹ˆë©´ null ë°˜í™˜
                {
                    return null;
                }
            }

            if (tableLines.Count >= 2) // í—¤ë” + ìµœì†Œ 1ê°œ í–‰ ì´ìƒì¼ ë•Œë§Œ í…Œì´ë¸”ë¡œ ì²˜ë¦¬
            {
                return new SemanticUnit
                {
                    Content = string.Join("\n", tableLines),
                    Position = startPosition,
                    SemanticWeight = 1.0, // í…Œì´ë¸”ì€ ë†’ì€ ê°€ì¤‘ì¹˜
                    ContextualRelevance = 1.0,
                    Importance = 0.9
                };
            }
        }

        return null;
    }

    private static List<string> CreateContextualChunks(
        List<SemanticUnit> units,
        int maxSize,
        double coherenceThreshold,
        ChunkingOptions options,
        bool adaptiveOverlap)
    {
        var chunks = new List<string>();
        var currentChunk = new List<SemanticUnit>();
        var currentSize = 0;
        var previousChunkText = string.Empty; // Phase 10: ì´ì „ ì²­í¬ ì „ì²´ í…ìŠ¤íŠ¸ ì €ì¥ (ì ì‘í˜• ì˜¤ë²„ë©ìš©)

        for (int i = 0; i < units.Count; i++)
        {
            var unit = units[i];
            var unitSize = EstimateTokenCount(unit.Content);

            // í…Œì´ë¸” SemanticUnit ê°ì§€ - ExtractSemanticUnitsì—ì„œ ìƒì„±ëœ í…Œì´ë¸” ë‹¨ìœ„ë¥¼ ë³´ì¡´
            var isTableUnit = unit.SemanticWeight >= 1.0 && unit.ContextualRelevance >= 1.0 &&
                              unit.Content.Contains('|') && unit.Content.Count(c => c == '|') >= 4; // ì™„ì „í•œ í…Œì´ë¸” íŒë‹¨


            if (isTableUnit)
            {
                // í˜„ì¬ ì²­í¬ê°€ ìˆê³  í…Œì´ë¸”ì„ ì¶”ê°€í•˜ë©´ í¬ê¸° ì´ˆê³¼ì¸ ê²½ìš° í˜„ì¬ ì²­í¬ ì™„ë£Œ
                if (currentChunk.Count != 0 && currentSize + unitSize > maxSize)
                {
                    var currentChunkText = string.Join(" ", currentChunk.Select(u => u.Content));
                    
                    // Phase 10: ì ì‘í˜• ì˜¤ë²„ë© ì ìš©
                    string overlapText = "";
                    if (adaptiveOverlap && !string.IsNullOrEmpty(previousChunkText))
                    {
                        var optimalOverlap = _overlapManager.CalculateOptimalOverlap(previousChunkText, currentChunkText, options);
                        overlapText = _overlapManager.CreateContextPreservingOverlap(previousChunkText, optimalOverlap);
                    }
                    else if (options.OverlapSize > 0)
                    {
                        var overlapUnits = GetOverlapUnits(currentChunk, options.OverlapSize, 1.0);
                        overlapText = string.Join(" ", overlapUnits.Select(u => u.Content));
                    }
                    
                    var chunkContent = CreateCoherentChunk(currentChunk, options.OverlapSize, overlapText);
                    chunks.Add(chunkContent);
                    
                    // ë‹¤ìŒ ì²­í¬ë¥¼ ìœ„í•´ í˜„ì¬ ì²­í¬ í…ìŠ¤íŠ¸ ì €ì¥
                    previousChunkText = currentChunkText;
                    
                    currentChunk.Clear();
                    currentSize = 0;
                }

                // í…Œì´ë¸” SemanticUnitì€ ìµœëŒ€ 2.5ë°°ê¹Œì§€ í—ˆìš©í•˜ì—¬ ì™„ì „ì„± ë³´ì¥
                if (unitSize <= maxSize * 2.5)
                {
                    currentChunk.Add(unit);
                    currentSize += unitSize;
                }
                else
                {
                    // ë§¤ìš° í° í…Œì´ë¸”ë§Œ ì˜ë¯¸ì  ê²½ê³„ì—ì„œ ë¶„í•  (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
                    var singleTableUnits = new List<SemanticUnit> { unit };
                    var tableParts = SplitLargeTable(singleTableUnits, maxSize);
                    foreach (var part in tableParts)
                    {
                        if (currentChunk.Count != 0)
                        {
                            var currentChunkText = string.Join(" ", currentChunk.Select(u => u.Content));
                            chunks.Add(CreateCoherentChunk(currentChunk, options.OverlapSize, ""));
                            previousChunkText = currentChunkText;
                            
                            currentChunk.Clear();
                            currentSize = 0;
                        }
                        chunks.Add(part);
                    }
                }
                continue;
            }

            // ì„¹ì…˜ í—¤ë” ê°ì§€ - ì„¹ì…˜ ê²½ê³„ì—ì„œ ì²­í‚¹
            var isSectionHeader = MarkdownSectionRegex.IsMatch(unit.Content);
            if (isSectionHeader && currentChunk.Count != 0 && currentSize > maxSize * 0.3) // 30% ì´ìƒì¼ ë•Œë§Œ ë¶„í• 
            {
                var currentChunkText = string.Join(" ", currentChunk.Select(u => u.Content));
                
                // Phase 10: ì ì‘í˜• ì˜¤ë²„ë© ì ìš©
                string overlapText = "";
                if (adaptiveOverlap && !string.IsNullOrEmpty(previousChunkText))
                {
                    var optimalOverlap = _overlapManager.CalculateOptimalOverlap(previousChunkText, currentChunkText, options);
                    overlapText = _overlapManager.CreateContextPreservingOverlap(previousChunkText, optimalOverlap);
                }
                
                var chunkContent = CreateCoherentChunk(currentChunk, options.OverlapSize, overlapText);
                chunks.Add(chunkContent);
                
                previousChunkText = currentChunkText;
                
                currentChunk.Clear();
                currentSize = 0;
            }

            // í…Œì´ë¸” í–‰ ë³´í˜¸ ë¡œì§ - í…Œì´ë¸” í–‰ì´ í¬í•¨ëœ ì²­í¬ëŠ” ë¬´ì¡°ê±´ ì™„ì„±
            var containsTableRow = ContainsTableRow(currentChunk) || IsTableRow(unit.Content);

            // Phase 10: ê²½ê³„ í’ˆì§ˆì„ ê³ ë ¤í•œ ì²­í‚¹ ê²°ì •
            if (currentChunk.Count != 0 &&
                currentSize + unitSize > maxSize &&
                !isSectionHeader && // ì„¹ì…˜ í—¤ë”ëŠ” ê°•ì œë¡œ í¬í•¨
                !containsTableRow && // í…Œì´ë¸” í–‰ì€ ë³´í˜¸
                ShouldStartNewChunk(currentChunk, unit, coherenceThreshold))
            {
                var currentChunkText = string.Join("\n", currentChunk.Select(u => u.Content));
                var entireText = string.Join("\n", units.Select(u => u.Content)); // ì „ì²´ í…ìŠ¤íŠ¸ ì¬êµ¬ì„±
                var proposedSplitPosition = unit.Position; // í˜„ì¬ ë¶„í•  ìœ„ì¹˜
                
                // ê²½ê³„ í’ˆì§ˆ í‰ê°€ ë° ê°œì„ 
                var boundaryResult = _boundaryQualityManager.EvaluateAndImproveBoundary(entireText, proposedSplitPosition, options);
                
                // í’ˆì§ˆì´ ê°œì„ ë˜ì—ˆë‹¤ë©´ ì¡°ì •ëœ ìœ„ì¹˜ ì‚¬ìš©
                if (boundaryResult.ImprovedPosition != proposedSplitPosition && boundaryResult.QualityScore > 0.7)
                {
                    // TODO: ì‹¤ì œë¡œëŠ” ê°œì„ ëœ ìœ„ì¹˜ì— ë”°ë¼ currentChunkì˜ ë‚´ìš©ì„ ì¡°ì •í•´ì•¼ í•¨
                    // í˜„ì¬ëŠ” ë¡œê¹…ë§Œ ìˆ˜í–‰í•˜ê³  ê¸°ì¡´ ë¡œì§ ìœ ì§€
                    // í–¥í›„ ì •êµí•œ ìœ„ì¹˜ ì¡°ì • ë¡œì§ êµ¬í˜„ ì˜ˆì •
                }
                
                var optimalOverlapSize = _overlapManager.CalculateOptimalOverlap(
                    previousChunkText, currentChunkText, options);
                var chunkContent = CreateCoherentChunk(currentChunk, optimalOverlapSize, previousChunkText);
                chunks.Add(chunkContent);

                // Store full chunk text for context preservation
                previousChunkText = chunkContent;
                currentChunk = new List<SemanticUnit>(); // Start fresh, overlap will be added via previousChunkText
                currentSize = 0;
            }

            currentChunk.Add(unit);
            currentSize += unitSize;
        }

        // ë§ˆì§€ë§‰ ì²­í¬ ì²˜ë¦¬
        if (currentChunk.Count != 0)
        {
            var optimalOverlapSize = _overlapManager.CalculateOptimalOverlap(
                previousChunkText, string.Join("\n", currentChunk.Select(u => u.Content)), options);
            var chunkContent = CreateCoherentChunk(currentChunk, optimalOverlapSize, previousChunkText);
            chunks.Add(chunkContent);
        }

        return chunks;
    }

    private static List<string> OptimizeChunks(List<string> chunks, double qualityThreshold, int maxSize)
    {
        var optimized = new List<string>();

        foreach (var chunk in chunks)
        {
            var quality = CalculateChunkQuality(chunk);

            // HARD LIMIT: í¬ê¸° ì´ˆê³¼ ì²­í¬ëŠ” í’ˆì§ˆì— ê´€ê³„ì—†ì´ ê°•ì œ ë¶„í• 
            if (chunk.Length > maxSize)
            {
                var forceSplit = EnforceMaxSize(chunk, maxSize);
                optimized.AddRange(forceSplit);
            }
            else if (quality >= qualityThreshold)
            {
                optimized.Add(chunk);
            }
            else
            {
                // í’ˆì§ˆì´ ë‚®ì€ ì²­í¬ëŠ” ë‹¤ì‹œ ë¶„í• í•˜ê±°ë‚˜ ì¸ì ‘ ì²­í¬ì™€ ë³‘í•©
                var reprocessed = ReprocessLowQualityChunk(chunk, maxSize);
                optimized.AddRange(reprocessed);
            }
        }

        return optimized;
    }

    private static bool ShouldStartNewChunk(List<SemanticUnit> currentChunk, SemanticUnit newUnit, double coherenceThreshold)
    {
        if (currentChunk.Count == 0)
            return false;

        var lastUnit = currentChunk.Last();
        var semanticDistance = CalculateSemanticDistance(lastUnit, newUnit);

        return semanticDistance > (1.0 - coherenceThreshold);
    }

    private static double CalculateSemanticDistance(SemanticUnit unit1, SemanticUnit unit2)
    {
        // ê°„ë‹¨í•œ ì˜ë¯¸ì  ê±°ë¦¬ ê³„ì‚° (ì‹¤ì œë¡œëŠ” ë” ì •êµí•œ NLP ê¸°ë²• í•„ìš”)
        var commonWords = GetCommonWords(unit1.Content, unit2.Content);
        var totalWords = GetUniqueWords(unit1.Content).Union(GetUniqueWords(unit2.Content)).Count();

        return totalWords == 0 ? 0.0 : 1.0 - (double)commonWords / totalWords;
    }

    private static DocumentChunk CreateIntelligentChunk(
        string content,
        DocumentMetadata metadata,
        int chunkIndex,
        int startPosition,
        ChunkingOptions options,
        double importanceWeighting,
        List<string> globalTechKeywords,
        string globalDocumentDomain)
    {
        var importance = CalculateImportance(content) * importanceWeighting;
        var contextQuality = CalculateChunkQuality(content);

        var chunk = new DocumentChunk
        {
            Id = Guid.NewGuid().ToString(),
            Content = content.Trim(),
            Metadata = metadata,
            StartPosition = startPosition,
            EndPosition = startPosition + content.Length,
            ChunkIndex = chunkIndex,
            Strategy = ChunkingStrategies.Intelligent,
            EstimatedTokens = EstimateTokenCount(content),
            CreatedAt = DateTime.UtcNow,
            Importance = importance,
            PageNumber = metadata.PageCount == 1 ? 1 : null,
            Properties = new Dictionary<string, object>
            {
                ["ContextQuality"] = contextQuality,
                ["SemanticCoherence"] = CalculateSemanticCoherence(content),
                ["StructuralElements"] = CountStructuralElements(content),
                ["ImportanceScore"] = importance
            }
        };

        // í’ˆì§ˆ ì ìˆ˜ ì‹¤ì œ ê³„ì‚°
        chunk.QualityScore = CalculateRealQualityScore(content, contextQuality);
        chunk.RelevanceScore = CalculateRelevanceScore(content, globalTechKeywords, globalDocumentDomain);
        chunk.InformationDensity = CalculateInformationDensity(content);
        
        // LLM ìµœì í™” ë©”íƒ€ë°ì´í„° ìë™ ìƒì„± (ì „ì—­ ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš©)
        EnhanceChunkForLlm(chunk, globalTechKeywords, globalDocumentDomain);

        // Phase 11: Vector Search Optimization
        ApplyVectorSearchOptimization(chunk, options);
        
        // Phase 12: Graph Search Optimization
        ApplyGraphSearchOptimization(chunk, options);

        return chunk;
    }

    /// <summary>
    /// ì²­í¬ë¥¼ LLMì— ìµœì í™”ëœ í˜•íƒœë¡œ ìë™ ê°•í™” (ì „ì—­ ì»¨í…ìŠ¤íŠ¸ ì‚¬ìš©)
    /// </summary>
    private static void EnhanceChunkForLlm(DocumentChunk chunk, List<string> globalTechKeywords, string globalDocumentDomain)
    {
        var content = chunk.Content;

        // 1. êµ¬ì¡°ì  ì»¨í…ìŠ¤íŠ¸ í—¤ë” ìƒì„±
        var contextParts = new List<string>();

        // ë¬¸ì„œ íƒ€ì… ì¶”ê°€
        if (!string.IsNullOrEmpty(chunk.Metadata.FileType))
            contextParts.Add($"Type: {chunk.Metadata.FileType}");

        // êµ¬ì¡°ì  ì—­í•  ì¶”ê°€ (í…Œì´ë¸”, ì½”ë“œ, ë¦¬ìŠ¤íŠ¸ ë“±)
        var structuralRole = DetectStructuralRole(content);
        if (structuralRole != "content")
            contextParts.Add($"Structure: {structuralRole}");

        // ì „ì—­ ê¸°ìˆ  í‚¤ì›Œë“œ ì‚¬ìš© (ì „ì²´ ë¬¸ì„œ ê¸°ë°˜)
        if (globalTechKeywords.Count != 0)
        {
            contextParts.Add($"Tech: {string.Join(", ", globalTechKeywords.Take(3))}");
            chunk.TechnicalKeywords = globalTechKeywords;
        }

        // ì „ì—­ ë¬¸ì„œ ë„ë©”ì¸ ì‚¬ìš© (ì „ì²´ ë¬¸ì„œ ê¸°ë°˜)
        chunk.DocumentDomain = globalDocumentDomain;
        if (chunk.DocumentDomain != "General")
            contextParts.Add($"Domain: {chunk.DocumentDomain}");

        // ContextualHeader ìƒì„±
        if (contextParts.Count != 0)
        {
            chunk.ContextualHeader = $"[{string.Join(" | ", contextParts)}]";
        }

        // êµ¬ì¡°ì  ì—­í•  ì„¤ì •
        chunk.StructuralRole = structuralRole;
    }

    /// <summary>
    /// êµ¬ì¡°ì  ì—­í•  ìë™ íƒì§€
    /// </summary>
    private static string DetectStructuralRole(string content)
    {
        var trimmed = content.Trim();

        // í—¤ë” (ë§ˆí¬ë‹¤ìš´ í—¤ë”)
        if (trimmed.StartsWith('#'))
            return "header";

        // í…Œì´ë¸”
        if (trimmed.Contains('|') && trimmed.Count(c => c == '|') >= 2)
            return "table";

        // ì½”ë“œ ë¸”ë¡
        if (trimmed.StartsWith("```") || trimmed.Contains("```"))
            return "code_block";

        // ë¦¬ìŠ¤íŠ¸
        if (trimmed.StartsWith("- ") || trimmed.StartsWith("* ") ||
            Regex.IsMatch(trimmed, @"^\d+\.\s"))
            return "list";

        return "content";
    }

    /// <summary>
    /// ê¸°ìˆ  í‚¤ì›Œë“œ ìë™ íƒì§€
    /// </summary>
    private static List<string> DetectTechnicalKeywords(string content)
    {
        var keywords = new List<string>();
        var text = content.ToLowerInvariant();

        var techPatterns = new Dictionary<string, string[]>
        {
            ["API"] = new[] { "api", "endpoint", "rest", "graphql" },
            ["Database"] = new[] { "database", "sql", "nosql", "query" },
            ["Frontend"] = new[] { "ui", "frontend", "react", "vue", "angular" },
            ["Backend"] = new[] { "server", "backend", "service", "microservice" },
            ["DevOps"] = new[] { "docker", "kubernetes", "deployment", "pipeline" },
            ["AI/ML"] = new[] { "ai", "ml", "model", "embedding", "vector" }
        };

        foreach (var (category, patterns) in techPatterns)
        {
            if (patterns.Any(pattern => ContainsWholeWord(text, pattern)))
                keywords.Add(category);
        }

        return keywords.Take(5).ToList(); // ìµœëŒ€ 5ê°œë¡œ ì œí•œ
    }

    /// <summary>
    /// ë‹¨ì–´ ê²½ê³„ë¥¼ ê³ ë ¤í•œ í‚¤ì›Œë“œ ê²€ì¶œ
    /// </summary>
    private static bool ContainsWholeWord(string text, string word)
    {
        var regex = new Regex($@"\b{Regex.Escape(word)}\b",
            RegexOptions.IgnoreCase);
        return regex.IsMatch(text);
    }

    /// <summary>
    /// ë¬¸ì„œ ë„ë©”ì¸ ìë™ íƒì§€
    /// </summary>
    private static string DetectDocumentDomain(string content, List<string> techKeywords)
    {
        var text = content.ToLowerInvariant();

        // Academic: í•™ìˆ  ê´€ë ¨ í‚¤ì›Œë“œ ìš°ì„  ì²´í¬ (íŠ¹ì •ì„±ì´ ë†’ìŒ)
        if (ContainsWholeWord(text, "research") || ContainsWholeWord(text, "study") ||
            ContainsWholeWord(text, "abstract") || ContainsWholeWord(text, "methodology") ||
            ContainsWholeWord(text, "ë…¼ë¬¸") || ContainsWholeWord(text, "literature") ||
            ContainsWholeWord(text, "theoretical") ||
            (ContainsWholeWord(text, "analysis") && (ContainsWholeWord(text, "research") || ContainsWholeWord(text, "data"))))
            return "Academic";

        // Business: ë¹„ì¦ˆë‹ˆìŠ¤ ê´€ë ¨ í‚¤ì›Œë“œ (ë” êµ¬ì²´ì ì¸ í‚¤ì›Œë“œë§Œ ì‚¬ìš©)
        if (ContainsWholeWord(text, "business") || ContainsWholeWord(text, "stakeholder") ||
            ContainsWholeWord(text, "strategy") || ContainsWholeWord(text, "strategic") ||
            ContainsWholeWord(text, "planning") || ContainsWholeWord(text, "timeline") ||
            ContainsWholeWord(text, "milestone") || ContainsWholeWord(text, "objective") ||
            (ContainsWholeWord(text, "requirement") && (ContainsWholeWord(text, "business") || ContainsWholeWord(text, "project"))) ||
            (ContainsWholeWord(text, "analysis") && ContainsWholeWord(text, "requirement")))
            return "Business";

        // Technical: ê¸°ìˆ  í‚¤ì›Œë“œê°€ 1ê°œ ì´ìƒì´ê±°ë‚˜ ê¸°ìˆ ì  ë‚´ìš©
        if (techKeywords.Count >= 1 ||
            ContainsWholeWord(text, "api") || ContainsWholeWord(text, "endpoint") ||
            ContainsWholeWord(text, "database") || ContainsWholeWord(text, "schema") ||
            ContainsWholeWord(text, "react") || ContainsWholeWord(text, "component") ||
            ContainsWholeWord(text, "function") || ContainsWholeWord(text, "class") || ContainsWholeWord(text, "method"))
            return "Technical";

        return "General";
    }

    // í—¬í¼ ë©”ì„œë“œë“¤
    private static List<string> ExtractSentences(string text)
    {
        return SentenceEndRegex.Split(text)
            .Where(s => !string.IsNullOrWhiteSpace(s))
            .Select(s => s.Trim())
            .Where(s => s.Length > 10) // ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ ì œì™¸
            .ToList();
    }

    private static double CalculateHeaderImportance(string header)
    {
        var level = header.TrimStart().TakeWhile(c => c == '#').Count();
        return level switch
        {
            1 => 1.0,
            2 => 0.9,
            3 => 0.8,
            4 => 0.7,
            5 => 0.6,
            6 => 0.5,
            _ => 0.8
        };
    }

    private static double CalculateParagraphImportance(string paragraph)
    {
        var hasKeywords = ImportantKeywordRegex.IsMatch(paragraph);
        var length = paragraph.Length;

        var importance = 0.5;
        if (hasKeywords) importance += 0.3;
        if (length > 200) importance += 0.1;
        if (length > 500) importance += 0.1;

        return Math.Min(importance, 1.0);
    }

    private static double CalculateSemanticWeight(string text)
    {
        // í‚¤ì›Œë“œ ë°€ë„, ë¬¸ì¥ ë³µì¡ë„ ë“±ì„ ê³ ë ¤í•œ ì˜ë¯¸ì  ê°€ì¤‘ì¹˜
        var keywordCount = ImportantKeywordRegex.Matches(text).Count;
        var wordCount = text.Split(' ', StringSplitOptions.RemoveEmptyEntries).Length;

        return wordCount == 0 ? 0.0 : Math.Min(1.0, 0.5 + (keywordCount * 0.1));
    }

    private static double CalculateContextualRelevance(string sentence, DocumentStructure structure)
    {
        // ë¬¸ì„œ êµ¬ì¡°ì™€ì˜ ì—°ê´€ì„± ê³„ì‚°
        var relevance = 0.5;

        if (structure.Headers.Any(h => sentence.Contains(h.Content.Replace("#", "").Trim(), StringComparison.OrdinalIgnoreCase)))
            relevance += 0.3;

        return Math.Min(relevance, 1.0);
    }

    private static double CalculateImportance(string content)
    {
        var importance = 0.5;

        if (ImportantKeywordRegex.IsMatch(content))
            importance += 0.2;

        if (content.Length > 100)
            importance += 0.1;

        if (HeaderRegex.IsMatch(content))
            importance += 0.2;

        return Math.Min(importance, 1.0);
    }

    private static string ExtractListItem(string text, int position)
    {
        var lines = text.Substring(position).Split('\n');
        return lines.FirstOrDefault()?.Trim() ?? string.Empty;
    }

    private static string CreateCoherentChunk(List<SemanticUnit> units, int overlapSize, string previousChunk = "")
    {
        var content = string.Join(" ", units.Select(u => u.Content));
        
        // Add adaptive overlap from previous chunk if provided
        if (!string.IsNullOrEmpty(previousChunk) && overlapSize > 0)
        {
            var contextPreservingOverlap = _overlapManager.CreateContextPreservingOverlap(previousChunk, overlapSize);
            
            // Ensure we don't duplicate content that's already in the units
            if (!string.IsNullOrEmpty(contextPreservingOverlap) && !content.StartsWith(contextPreservingOverlap))
            {
                content = contextPreservingOverlap + " " + content;
            }
        }
        
        return content;
    }

    /// <summary>
    /// ì£¼ì–´ì§„ ì²­í¬ì— í…Œì´ë¸” í–‰ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
    /// </summary>
    private static bool ContainsTableRow(List<SemanticUnit> chunk)
    {
        return chunk.Any(unit => IsTableRow(unit.Content));
    }

    /// <summary>
    /// ì£¼ì–´ì§„ ì½˜í…ì¸ ê°€ í…Œì´ë¸” í–‰ì¸ì§€ í™•ì¸
    /// </summary>
    private static bool IsTableRow(string content)
    {
        return content.Contains('|') && content.Count(c => c == '|') >= 2;
    }

    /// <summary>
    /// SemanticUnits ì¤‘ì— í…Œì´ë¸”ì´ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
    /// </summary>
    private static bool ContainsAnyTable(List<SemanticUnit> units)
    {
        return units.Any(unit => IsTableRow(unit.Content));
    }

    private static List<SemanticUnit> GetOverlapUnits(List<SemanticUnit> currentChunk, int targetOverlapSize, double relevance)
    {
        if (targetOverlapSize <= 0 || currentChunk.Count == 0)
            return new List<SemanticUnit>();
        
        // Calculate overlap size based on characters/tokens, not unit count
        var adjustedOverlapSize = (int)(targetOverlapSize * relevance);
        var overlapUnits = new List<SemanticUnit>();
        var currentOverlapSize = 0;
        
        // Add units from the end until we reach the target overlap size
        for (int i = currentChunk.Count - 1; i >= 0; i--)
        {
            var unit = currentChunk[i];
            var unitSize = EstimateTokenCount(unit.Content);
            
            if (currentOverlapSize + unitSize > adjustedOverlapSize * 1.5) // Allow 50% overshoot
                break;
                
            overlapUnits.Insert(0, unit); // Insert at beginning to maintain order
            currentOverlapSize += unitSize;
            
            if (currentOverlapSize >= adjustedOverlapSize)
                break;
        }
        
        return overlapUnits;
    }

    private static double CalculateChunkQuality(string chunk)
    {
        var sentences = ExtractSentences(chunk);
        if (sentences.Count == 0) return 0.0;

        var avgSentenceLength = sentences.Average(s => s.Length);
        var hasKeywords = ImportantKeywordRegex.IsMatch(chunk);
        var coherence = CalculateSemanticCoherence(chunk);

        var quality = 0.3 * (avgSentenceLength > 50 ? 1.0 : avgSentenceLength / 50.0) +
                     0.2 * (hasKeywords ? 1.0 : 0.0) +
                     0.5 * coherence;

        return Math.Min(quality, 1.0);
    }

    private static double CalculateSemanticCoherence(string content)
    {
        // ê°„ë‹¨í•œ ì‘ì§‘ì„± ì¸¡ì • - ë°˜ë³µë˜ëŠ” í‚¤ì›Œë“œì˜ ë¹„ìœ¨
        var words = content.ToLowerInvariant()
            .Split(' ', StringSplitOptions.RemoveEmptyEntries)
            .Where(w => w.Length > 3)
            .ToList();

        if (words.Count == 0) return 0.0;

        var wordFreq = words.GroupBy(w => w).ToDictionary(g => g.Key, g => g.Count());
        var repeatedWords = wordFreq.Where(kvp => kvp.Value > 1).Count();

        return (double)repeatedWords / words.Distinct().Count();
    }

    private static int CountStructuralElements(string content)
    {
        return HeaderRegex.Matches(content).Count +
               ListItemRegex.Matches(content).Count +
               ParagraphRegex.Matches(content).Count;
    }

    private static List<string> ReprocessLowQualityChunk(string chunk, int maxSize)
    {
        // ì €í’ˆì§ˆ ì²­í¬ ì¬ì²˜ë¦¬ ë¡œì§
        if (chunk.Length <= maxSize / 2)
        {
            return new List<string> { chunk }; // ë„ˆë¬´ ì§§ìœ¼ë©´ ê·¸ëŒ€ë¡œ ìœ ì§€
        }

        // ë¬¸ì¥ ë‹¨ìœ„ë¡œ ì¬ë¶„í• 
        var sentences = ExtractSentences(chunk);
        var result = new List<string>();
        var currentPart = new List<string>();
        var currentLength = 0;

        foreach (var sentence in sentences)
        {
            if (currentLength + sentence.Length > maxSize && currentPart.Count != 0)
            {
                result.Add(string.Join(" ", currentPart));
                currentPart.Clear();
                currentLength = 0;
            }

            currentPart.Add(sentence);
            currentLength += sentence.Length;
        }

        if (currentPart.Count != 0)
        {
            result.Add(string.Join(" ", currentPart));
        }

        return result;
    }

    private static int GetCommonWords(string text1, string text2)
    {
        var words1 = GetUniqueWords(text1);
        var words2 = GetUniqueWords(text2);
        return words1.Intersect(words2).Count();
    }

    private static HashSet<string> GetUniqueWords(string text)
    {
        return text.ToLowerInvariant()
            .Split(' ', StringSplitOptions.RemoveEmptyEntries)
            .Where(w => w.Length > 3)
            .ToHashSet();
    }

    private static int EstimateTokenCount(string text)
    {
        if (string.IsNullOrWhiteSpace(text))
            return 0;

        var words = text.Split(' ', StringSplitOptions.RemoveEmptyEntries).Length;
        return (int)(words * 1.3);
    }

    /// <summary>
    /// í¬ê¸° ì´ˆê³¼ ì²­í¬ë¥¼ ê°•ì œë¡œ MaxSize ì´í•˜ë¡œ ë¶„í• 
    /// ì˜ë¯¸ì  ê²½ê³„(ë¬¸ì¥, ë‹¨ì–´)ë¥¼ ê³ ë ¤í•˜ì—¬ ìµœëŒ€í•œ ìì—°ìŠ¤ëŸ½ê²Œ ë¶„í• 
    /// </summary>
    private static List<string> EnforceMaxSize(string chunk, int maxSize)
    {
        var result = new List<string>();

        if (chunk.Length <= maxSize)
        {
            result.Add(chunk);
            return result;
        }

        // ğŸ”¥ í…Œì´ë¸” ë³´í˜¸ ë¡œì§: TABLE_START/ENDê°€ í¬í•¨ëœ ê²½ìš° íŠ¹ë³„ ì²˜ë¦¬
        if (chunk.Contains("<!-- TABLE_START -->") && chunk.Contains("<!-- TABLE_END -->"))
        {
            return EnforceMaxSizeForTable(chunk, maxSize);
        }

        // ğŸ”¥ ë‹¤ì¤‘ í…Œì´ë¸” í–‰ ë³´í˜¸: | ë¬¸ìê°€ ë§ì€ ê²½ìš° (í…Œì´ë¸”ë¡œ ì¶”ì •)
        var tableRowCount = chunk.Split('\n', StringSplitOptions.RemoveEmptyEntries)
            .Count(line => line.Contains('|') && line.Count(c => c == '|') >= 2);
        
        if (tableRowCount >= 3) // 3ê°œ ì´ìƒì˜ í…Œì´ë¸” í–‰ì´ ìˆìœ¼ë©´ í…Œì´ë¸”ë¡œ ê°„ì£¼
        {
            return EnforceMaxSizeForTable(chunk, maxSize);
        }

        // 1ë‹¨ê³„: ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í•  ì‹œë„
        var sentences = ExtractSentences(chunk);
        var currentPart = new List<string>();
        var currentLength = 0;

        foreach (var sentence in sentences)
        {
            // ë‹¨ì¼ ë¬¸ì¥ì´ maxSizeë¥¼ ì´ˆê³¼í•˜ëŠ” ê²½ìš° ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„í• 
            if (sentence.Length > maxSize)
            {
                // í˜„ì¬ ëˆ„ì ëœ ë¶€ë¶„ì´ ìˆìœ¼ë©´ ë¨¼ì € ì¶”ê°€
                if (currentPart.Count != 0)
                {
                    result.Add(string.Join(" ", currentPart));
                    currentPart.Clear();
                    currentLength = 0;
                }

                // ê¸´ ë¬¸ì¥ì„ ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„í• 
                var wordChunks = SplitByWords(sentence, maxSize);
                result.AddRange(wordChunks);
            }
            else if (currentLength + sentence.Length + 1 > maxSize && currentPart.Count != 0) // +1 for space
            {
                // í˜„ì¬ ëˆ„ì  ë¶„ëŸ‰ì´ ì´ˆê³¼ë  ê²½ìš°
                result.Add(string.Join(" ", currentPart));
                currentPart.Clear();
                currentPart.Add(sentence);
                currentLength = sentence.Length;
            }
            else
            {
                // ì •ìƒì ìœ¼ë¡œ ì¶”ê°€
                currentPart.Add(sentence);
                currentLength += sentence.Length + (currentPart.Count > 1 ? 1 : 0); // space
            }
        }

        // ë§ˆì§€ë§‰ ë¶€ë¶„ ì¶”ê°€
        if (currentPart.Any())
        {
            result.Add(string.Join(" ", currentPart));
        }

        return result;
    }

    /// <summary>
    /// ê¸´ ë¬¸ì¥ì„ ë‹¨ì–´ ë‹¨ìœ„ë¡œ maxSize ì´í•˜ì˜ ì²­í¬ë¡œ ë¶„í• 
    /// </summary>
    private static List<string> SplitByWords(string text, int maxSize)
    {
        var result = new List<string>();
        var words = text.Split(' ', StringSplitOptions.RemoveEmptyEntries);
        var currentChunk = new List<string>();
        var currentLength = 0;

        foreach (var word in words)
        {
            if (currentLength + word.Length + 1 > maxSize && currentChunk.Any()) // +1 for space
            {
                result.Add(string.Join(" ", currentChunk));
                currentChunk.Clear();
                currentChunk.Add(word);
                currentLength = word.Length;
            }
            else
            {
                currentChunk.Add(word);
                currentLength += word.Length + (currentChunk.Count > 1 ? 1 : 0); // space
            }
        }

        if (currentChunk.Any())
        {
            result.Add(string.Join(" ", currentChunk));
        }

        return result;
    }

    /// <summary>
    /// í…Œì´ë¸” ì „ìš© í¬ê¸° ê°•ì œ ë¶„í•  - í…Œì´ë¸” êµ¬ì¡°ì™€ í—¤ë” ë³´ì¡´ì— ìµœì í™”
    /// </summary>
    private static List<string> EnforceMaxSizeForTable(string tableChunk, int maxSize)
    {
        var result = new List<string>();

        // í…Œì´ë¸”ì´ ì ì ˆí•œ í¬ê¸°ë©´ ê·¸ëŒ€ë¡œ ìœ ì§€ (ìµœëŒ€ 3ë°°ê¹Œì§€ í—ˆìš©)
        if (tableChunk.Length <= maxSize * 3)
        {
            result.Add(tableChunk);
            return result;
        }

        // í…Œì´ë¸” ë§ˆì»¤ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• 
        if (tableChunk.Contains("<!-- TABLE_START -->") && tableChunk.Contains("<!-- TABLE_END -->"))
        {
            return SplitTableByMarkers(tableChunk, maxSize);
        }

        // ë§ˆì»¤ê°€ ì—†ëŠ” ê²½ìš° í…Œì´ë¸” í–‰ ë‹¨ìœ„ë¡œ ë¶„í• 
        return SplitTableByRows(tableChunk, maxSize);
    }

    /// <summary>
    /// í…Œì´ë¸” ë§ˆì»¤ë¥¼ ê¸°ì¤€ìœ¼ë¡œ í…Œì´ë¸”ì„ ë¶„í• 
    /// </summary>
    private static List<string> SplitTableByMarkers(string tableChunk, int maxSize)
    {
        var result = new List<string>();
        var lines = tableChunk.Split('\n', StringSplitOptions.None);
        
        var headerLines = new List<string>();
        var currentTableLines = new List<string>();
        var isInTable = false;
        var tableStartFound = false;

        foreach (var line in lines)
        {
            if (line.Contains("<!-- TABLE_START -->"))
            {
                isInTable = true;
                tableStartFound = true;
                currentTableLines.Add(line);
                continue;
            }

            if (line.Contains("<!-- TABLE_END -->"))
            {
                currentTableLines.Add(line);
                
                // ì „ì²´ í…Œì´ë¸” í¬ê¸° í™•ì¸
                var completeTable = string.Join("\n", currentTableLines);
                if (completeTable.Length <= maxSize * 2) // 2ë°°ê¹Œì§€ í—ˆìš©
                {
                    result.Add(completeTable);
                }
                else
                {
                    // í…Œì´ë¸”ì´ ë„ˆë¬´ í¬ë©´ í–‰ ë‹¨ìœ„ë¡œ ë¶„í• 
                    var splitTables = SplitLargeTableContent(currentTableLines, maxSize);
                    result.AddRange(splitTables);
                }
                
                currentTableLines.Clear();
                isInTable = false;
                continue;
            }

            if (isInTable)
            {
                currentTableLines.Add(line);
            }
            else if (!tableStartFound)
            {
                // í…Œì´ë¸” ì‹œì‘ ì „ì˜ ë‚´ìš©
                headerLines.Add(line);
            }
        }

        // í…Œì´ë¸”ì´ ì™„ë£Œë˜ì§€ ì•Šì€ ê²½ìš° ì²˜ë¦¬
        if (currentTableLines.Any())
        {
            var incompleteTable = string.Join("\n", currentTableLines);
            result.Add(incompleteTable);
        }

        return result;
    }

    /// <summary>
    /// ë§ˆì»¤ ì—†ëŠ” í…Œì´ë¸”ì„ í–‰ ë‹¨ìœ„ë¡œ ë¶„í• 
    /// </summary>
    private static List<string> SplitTableByRows(string tableChunk, int maxSize)
    {
        var result = new List<string>();
        var lines = tableChunk.Split('\n', StringSplitOptions.None);
        
        // í—¤ë”ì™€ êµ¬ë¶„ì ì‹ë³„
        var headerLine = "";
        var separatorLine = "";
        var dataLines = new List<string>();
        
        for (int i = 0; i < lines.Length; i++)
        {
            var line = lines[i];
            if (line.Contains('|') && line.Count(c => c == '|') >= 2)
            {
                if (string.IsNullOrEmpty(headerLine))
                {
                    headerLine = line;
                }
                else if (string.IsNullOrEmpty(separatorLine) && (line.Contains("---") || line.Contains(":-")))
                {
                    separatorLine = line;
                }
                else
                {
                    dataLines.Add(line);
                }
            }
        }

        if (string.IsNullOrEmpty(headerLine))
        {
            // í…Œì´ë¸”ì´ ì•„ë‹Œ ê²ƒìœ¼ë¡œ íŒë‹¨í•˜ê³  ì¼ë°˜ ë¶„í• 
            return SplitByWords(tableChunk, maxSize);
        }

        // í—¤ë” + êµ¬ë¶„ì í¬ê¸°
        var headerSize = (headerLine + "\n" + separatorLine).Length;
        var currentLines = new List<string>();
        var currentSize = headerSize;

        foreach (var dataLine in dataLines)
        {
            if (currentSize + dataLine.Length + 1 > maxSize && currentLines.Any()) // +1 for newline
            {
                // í˜„ì¬ í…Œì´ë¸” íŒŒíŠ¸ ì™„ì„±
                var tableContent = headerLine + "\n" + separatorLine + "\n" + string.Join("\n", currentLines);
                result.Add(tableContent);
                
                // ë‹¤ìŒ íŒŒíŠ¸ ì‹œì‘
                currentLines.Clear();
                currentSize = headerSize;
            }
            
            currentLines.Add(dataLine);
            currentSize += dataLine.Length + 1;
        }

        // ë§ˆì§€ë§‰ íŒŒíŠ¸
        if (currentLines.Any())
        {
            var tableContent = headerLine + "\n" + separatorLine + "\n" + string.Join("\n", currentLines);
            result.Add(tableContent);
        }

        return result;
    }

    /// <summary>
    /// í° í…Œì´ë¸” ë‚´ìš©ì„ ë¶„í• 
    /// </summary>
    private static List<string> SplitLargeTableContent(List<string> tableLines, int maxSize)
    {
        var result = new List<string>();
        
        // TABLE_STARTì™€ TABLE_ENDë¥¼ ì°¾ê¸°
        var startIndex = tableLines.FindIndex(line => line.Contains("<!-- TABLE_START -->"));
        var endIndex = tableLines.FindIndex(line => line.Contains("<!-- TABLE_END -->"));
        
        if (startIndex == -1 || endIndex == -1)
        {
            // ë§ˆì»¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìœ¼ë©´ ì „ì²´ ë°˜í™˜
            result.Add(string.Join("\n", tableLines));
            return result;
        }

        var headerAndStart = new List<string>();
        var dataLines = new List<string>();
        var endMarker = tableLines[endIndex];

        // í—¤ë”ì™€ êµ¬ë¶„ì ìˆ˜ì§‘ (ì²˜ìŒ 2-3ì¤„)
        for (int i = startIndex; i < Math.Min(startIndex + 3, endIndex); i++)
        {
            headerAndStart.Add(tableLines[i]);
        }

        // ë°ì´í„° í–‰ë“¤ ìˆ˜ì§‘
        for (int i = startIndex + 3; i < endIndex; i++)
        {
            if (i < tableLines.Count)
            {
                dataLines.Add(tableLines[i]);
            }
        }

        var headerSize = string.Join("\n", headerAndStart).Length;
        var currentLines = new List<string>();
        var currentSize = headerSize;

        foreach (var dataLine in dataLines)
        {
            if (currentSize + dataLine.Length + 1 > maxSize && currentLines.Any())
            {
                // í˜„ì¬ í…Œì´ë¸” íŒŒíŠ¸ ì™„ì„±
                var tablePart = new List<string>();
                tablePart.AddRange(headerAndStart);
                tablePart.AddRange(currentLines);
                tablePart.Add(endMarker);
                
                result.Add(string.Join("\n", tablePart));
                
                // ë‹¤ìŒ íŒŒíŠ¸ ì‹œì‘
                currentLines.Clear();
                currentSize = headerSize;
            }
            
            currentLines.Add(dataLine);
            currentSize += dataLine.Length + 1;
        }

        // ë§ˆì§€ë§‰ íŒŒíŠ¸
        if (currentLines.Any())
        {
            var tablePart = new List<string>();
            tablePart.AddRange(headerAndStart);
            tablePart.AddRange(currentLines);
            tablePart.Add(endMarker);
            
            result.Add(string.Join("\n", tablePart));
        }

        return result;
    }

    private static T GetStrategyOption<T>(ChunkingOptions options, string key, T defaultValue)
    {
        // ì˜µì…˜ ë‹¨ìˆœí™”: í•­ìƒ ê¸°ë³¸ê°’ ì‚¬ìš© (ìµœê³  í’ˆì§ˆ ê¸°ë³¸ ì„¤ì •)
        return defaultValue;
    }

    /// <summary>
    /// í…Œì´ë¸” ì‹œì‘ ì—¬ë¶€ í™•ì¸
    /// </summary>
    private static bool IsTableStart(string content)
    {
        return TableRegex.IsMatch(content) || content.Contains('|') && content.Count(c => c == '|') >= 3;
    }

    /// <summary>
    /// ì™„ì „í•œ í…Œì´ë¸”ì„ ì¶”ì¶œ (í—¤ë” + êµ¬ë¶„ì + ë°ì´í„° í–‰ë“¤)
    /// </summary>
    private static List<SemanticUnit> ExtractCompleteTable(List<SemanticUnit> units, int startIndex, out int endIndex)
    {
        var tableUnits = new List<SemanticUnit>();
        endIndex = startIndex;

        for (int i = startIndex; i < units.Count; i++)
        {
            var content = units[i].Content;

            // í…Œì´ë¸” í–‰ì¸ì§€ í™•ì¸ (|ë¡œ êµ¬ë¶„ë˜ëŠ” êµ¬ì¡°)
            if (content.Contains('|') && (content.Count(c => c == '|') >= 2 || TableSeparatorRegex.IsMatch(content)))
            {
                tableUnits.Add(units[i]);
                endIndex = i;
            }
            else if (tableUnits.Any()) // í…Œì´ë¸”ì´ ì‹œì‘ëœ í›„ í…Œì´ë¸”ì´ ì•„ë‹Œ í–‰ì„ ë§Œë‚˜ë©´ ì¢…ë£Œ
            {
                break;
            }
        }

        return tableUnits;
    }

    /// <summary>
    /// ëŒ€ìš©ëŸ‰ í…Œì´ë¸”ì„ ì˜ë¯¸ì  ê²½ê³„ì—ì„œ ë¶„í• 
    /// </summary>
    private static List<string> SplitLargeTable(List<SemanticUnit> tableUnits, int maxSize)
    {
        var result = new List<string>();

        if (!tableUnits.Any())
            return result;

        // í—¤ë”ì™€ êµ¬ë¶„ì ì‹ë³„
        var headerUnit = tableUnits.FirstOrDefault();
        var separatorUnit = tableUnits.Skip(1).FirstOrDefault();
        var dataRows = tableUnits.Skip(2).ToList(); // ì‹¤ì œ ë°ì´í„° í–‰ë“¤ë§Œ

        // í—¤ë” + êµ¬ë¶„ì í¬ê¸°
        var headerSize = EstimateTokenCount(headerUnit?.Content ?? "") + EstimateTokenCount(separatorUnit?.Content ?? "");

        var currentRows = new List<SemanticUnit>();
        var currentSize = headerSize; // í—¤ë” í¬ê¸°ë¶€í„° ì‹œì‘

        foreach (var row in dataRows)
        {
            var rowSize = EstimateTokenCount(row.Content);

            // í—¤ë” í¬í•¨ ì‹œ í¬ê¸° ì´ˆê³¼í•˜ë©´ ìƒˆë¡œìš´ í…Œì´ë¸” íŒŒíŠ¸ ìƒì„± (ìµœì†Œ 1í–‰ì€ í¬í•¨)
            if (currentRows.Any() && currentSize + rowSize > maxSize)
            {
                // í˜„ì¬ íŒŒíŠ¸ ì™„ì„±: í—¤ë” + êµ¬ë¶„ì + ë°ì´í„° í–‰ë“¤
                var partUnits = new List<SemanticUnit>();
                if (headerUnit != null) partUnits.Add(headerUnit);
                if (separatorUnit != null) partUnits.Add(separatorUnit);
                partUnits.AddRange(currentRows);

                result.Add(CreateCoherentChunk(partUnits, 0));

                // ë‹¤ìŒ íŒŒíŠ¸ ì‹œì‘
                currentRows.Clear();
                currentSize = headerSize;
            }

            currentRows.Add(row);
            currentSize += rowSize;
        }

        // ë§ˆì§€ë§‰ íŒŒíŠ¸ ì²˜ë¦¬
        if (currentRows.Any())
        {
            var partUnits = new List<SemanticUnit>();
            if (headerUnit != null) partUnits.Add(headerUnit);
            if (separatorUnit != null) partUnits.Add(separatorUnit);
            partUnits.AddRange(currentRows);

            result.Add(CreateCoherentChunk(partUnits, 0));
        }

        return result;
    }

    // ë‚´ë¶€ í´ë˜ìŠ¤ë“¤
    private class DocumentStructure
    {
        public List<StructuralElement> Headers { get; set; } = new();
        public List<StructuralElement> ListItems { get; set; } = new();
        public List<StructuralElement> Paragraphs { get; set; } = new();
    }

    private class StructuralElement
    {
        public string Content { get; set; } = string.Empty;
        public int Position { get; set; }
        public string Type { get; set; } = string.Empty;
        public double Importance { get; set; }
    }

    private class SemanticUnit
    {
        public string Content { get; set; } = string.Empty;
        public int Position { get; set; }
        public double SemanticWeight { get; set; }
        public double ContextualRelevance { get; set; }
        public double Importance { get; set; }
    }

    /// <summary>
    /// ì‹¤ì œ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (Mockì´ ì•„ë‹Œ ì‹¤ì œ ì•Œê³ ë¦¬ì¦˜)
    /// </summary>
    private static double CalculateRealQualityScore(string content, double contextQuality)
    {
        if (string.IsNullOrWhiteSpace(content)) return 0.0;

        var scores = new List<double>();

        // 1. í…ìŠ¤íŠ¸ ì™„ì„±ë„ ì ìˆ˜ (0.0-1.0)
        var completenessScore = CalculateCompletenessScore(content);
        scores.Add(completenessScore);

        // 2. êµ¬ì¡°ì  ì¼ê´€ì„± ì ìˆ˜ (0.0-1.0) 
        var structuralScore = CalculateStructuralConsistency(content);
        scores.Add(structuralScore);

        // 3. ì •ë³´ ë°€ë„ ì í•©ì„± ì ìˆ˜ (0.0-1.0)
        var densityScore = Math.Min(CalculateInformationDensity(content), 1.0);
        scores.Add(densityScore);

        // 4. ì»¨í…ìŠ¤íŠ¸ í’ˆì§ˆ ì ìˆ˜ (ì´ë¯¸ ê³„ì‚°ëœ ê°’)
        scores.Add(Math.Min(contextQuality, 1.0));

        // ê°€ì¤‘ í‰ê·  ê³„ì‚°
        var qualityScore = scores.Average();
        
        return Math.Max(0.0, Math.Min(1.0, qualityScore));
    }

    /// <summary>
    /// ë¬¸ì„œ ë§¥ë½ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
    /// </summary>
    private static double CalculateRelevanceScore(string content, List<string> globalTechKeywords, string documentDomain)
    {
        if (string.IsNullOrWhiteSpace(content)) return 0.0;

        var scores = new List<double>();

        // 1. ê¸°ìˆ  í‚¤ì›Œë“œ ê´€ë ¨ì„± (0.0-1.0)
        var keywordRelevance = CalculateKeywordRelevance(content, globalTechKeywords);
        scores.Add(keywordRelevance);

        // 2. ë„ë©”ì¸ ì í•©ì„± (0.0-1.0)
        var domainRelevance = CalculateDomainRelevance(content, documentDomain);
        scores.Add(domainRelevance);

        // 3. ì˜ë¯¸ì  ì¼ê´€ì„± (0.0-1.0)
        var semanticCoherence = CalculateSemanticCoherence(content);
        scores.Add(semanticCoherence);

        return scores.Average();
    }

    /// <summary>
    /// ì •ë³´ ë°€ë„ ê³„ì‚° (ë‹¨ìœ„ ê¸¸ì´ë‹¹ ì •ë³´ëŸ‰)
    /// </summary>
    private static double CalculateInformationDensity(string content)
    {
        if (string.IsNullOrWhiteSpace(content)) return 0.0;

        var contentLength = content.Length;
        if (contentLength == 0) return 0.0;

        // ì •ë³´ ìš”ì†Œ ê³„ì‚°
        var sentences = ExtractSentences(content).Count;
        var technicalTerms = CountTechnicalTerms(content);
        var structuralElements = CountStructuralElements(content);
        var uniqueWords = content.ToLowerInvariant()
            .Split(separator, StringSplitOptions.RemoveEmptyEntries)
            .Where(w => w.Length > 3)
            .Distinct()
            .Count();

        // ì •ë³´ ë°€ë„ = (ë¬¸ì¥ìˆ˜ + ê¸°ìˆ ìš©ì–´ + êµ¬ì¡°ìš”ì†Œ + ê³ ìœ ë‹¨ì–´ìˆ˜) / ë¬¸ììˆ˜ * 1000
        var informationScore = (sentences + technicalTerms + structuralElements + uniqueWords) * 1000.0 / contentLength;

        // 0.0-1.0 ë²”ìœ„ë¡œ ì •ê·œí™” (ê²½í—˜ì  ê¸°ì¤€: 50 ì´ìƒì€ 1.0)
        return Math.Min(informationScore / 50.0, 1.0);
    }

    /// <summary>
    /// í…ìŠ¤íŠ¸ ì™„ì„±ë„ ì ìˆ˜ ê³„ì‚°
    /// </summary>
    private static double CalculateCompletenessScore(string content)
    {
        if (string.IsNullOrWhiteSpace(content)) return 0.0;

        var scores = new List<double>();

        // 1. ë¬¸ì¥ ì™„ì„±ë„ - ë¬¸ì¥ì´ ì™„ì „í•œì§€ í™•ì¸
        var sentences = ExtractSentences(content);
        var completeSentences = sentences.Count(s => s.Trim().EndsWith(".") || s.Trim().EndsWith("!") || s.Trim().EndsWith("?"));
        var sentenceCompleteness = sentences.Count > 0 ? (double)completeSentences / sentences.Count : 0.0;
        scores.Add(sentenceCompleteness);

        // 2. êµ¬ì¡°ì  ì™„ì„±ë„ - í‘œ, ë¦¬ìŠ¤íŠ¸ ë“±ì´ ì™„ì „í•œì§€ í™•ì¸
        var structuralCompleteness = CalculateStructuralCompleteness(content);
        scores.Add(structuralCompleteness);

        // 3. ê¸¸ì´ ì í•©ì„± - ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸¸ì§€ ì•Šì€ì§€ í™•ì¸
        var lengthApproppriateness = CalculateLengthAppropriateness(content.Length);
        scores.Add(lengthApproppriateness);

        return scores.Average();
    }

    /// <summary>
    /// êµ¬ì¡°ì  ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚°
    /// </summary>
    private static double CalculateStructuralConsistency(string content)
    {
        if (string.IsNullOrWhiteSpace(content)) return 0.0;

        var scores = new List<double>();

        // 1. í—¤ë” ê³„ì¸µ ì¼ê´€ì„±
        var headerConsistency = CalculateHeaderConsistency(content);
        scores.Add(headerConsistency);

        // 2. ë¦¬ìŠ¤íŠ¸ êµ¬ì¡° ì¼ê´€ì„±  
        var listConsistency = CalculateListConsistency(content);
        scores.Add(listConsistency);

        // 3. í…Œì´ë¸” êµ¬ì¡° ì¼ê´€ì„±
        var tableConsistency = CalculateTableConsistency(content);
        scores.Add(tableConsistency);

        return scores.Average();
    }

    // ë³´ì¡° ë©”ì„œë“œë“¤
    private static double CalculateKeywordRelevance(string content, List<string> keywords)
    {
        if (keywords == null || keywords.Count == 0) return 0.5;

        var contentLower = content.ToLowerInvariant();
        var matchedKeywords = keywords.Count(k => contentLower.Contains(k.ToLowerInvariant()));
        
        return (double)matchedKeywords / keywords.Count;
    }

    private static double CalculateDomainRelevance(string content, string domain)
    {
        if (string.IsNullOrWhiteSpace(domain)) return 0.5;

        var domainKeywords = GetDomainSpecificKeywords(domain);
        return CalculateKeywordRelevance(content, domainKeywords);
    }

    private static List<string> GetDomainSpecificKeywords(string domain)
    {
        return domain.ToLowerInvariant() switch
        {
            "technical" => new List<string> { "system", "technology", "implementation", "architecture", "design", "development" },
            "business" => new List<string> { "requirement", "process", "workflow", "business", "stakeholder", "objective" },
            "academic" => new List<string> { "research", "analysis", "methodology", "conclusion", "hypothesis", "evidence" },
            _ => new List<string> { "information", "content", "data", "process", "system" }
        };
    }

    private static int CountTechnicalTerms(string content)
    {
        var technicalPatterns = new[] { 
            @"\b[A-Z]{2,}\b", // ì•½ì–´ (API, HTTP ë“±)
            @"\b\w+\.\w+\b", // ë„¤ì„ìŠ¤í˜ì´ìŠ¤/ë„ë©”ì¸ í˜•íƒœ
            @"\b\w*(?:Service|Manager|Controller|Repository|Factory|Builder|Strategy)\b" // ì¼ë°˜ì ì¸ íŒ¨í„´
        };

        return technicalPatterns.Sum(pattern => Regex.Matches(content, pattern).Count);
    }

    private static double CalculateStructuralCompleteness(string content)
    {
        var scores = new List<double>();

        // í…Œì´ë¸” ì™„ì„±ë„
        if (content.Contains('|'))
        {
            var tableComplete = content.Contains("TABLE_START") && content.Contains("TABLE_END") ? 1.0 : 0.8;
            scores.Add(tableComplete);
        }

        // ë¦¬ìŠ¤íŠ¸ ì™„ì„±ë„
        if (content.Contains('â€¢') || content.Contains('-') || Regex.IsMatch(content, @"^\d+\."))
        {
            var listComplete = content.Contains("LIST_START") && content.Contains("LIST_END") ? 1.0 : 0.8;
            scores.Add(listComplete);
        }

        // ì½”ë“œ ë¸”ë¡ ì™„ì„±ë„
        if (content.Contains("```"))
        {
            var codeComplete = content.Contains("CODE_START") && content.Contains("CODE_END") ? 1.0 : 0.8;
            scores.Add(codeComplete);
        }

        return scores.Count != 0 ? scores.Average() : 1.0; // êµ¬ì¡°ì  ìš”ì†Œê°€ ì—†ìœ¼ë©´ ì™„ì „í•˜ë‹¤ê³  ê°„ì£¼
    }

    private static double CalculateLengthAppropriateness(int length)
    {
        // Phase 15: ì ì ˆí•œ ì²­í¬ ê¸¸ì´ ë²”ìœ„ ìµœì í™”: 300-1500ì
        if (length < 150) return 0.3; // ë„ˆë¬´ ì§§ìŒ
        if (length < 300) return 0.6; // ì§§ìŒ
        if (length <= 1500) return 1.0; // ì ì ˆí•¨
        if (length <= 2000) return 0.8; // ë‹¤ì†Œ ê¸¸ìŒ
        return 0.6; // ë„ˆë¬´ ê¸¸ìŒ
    }

    /// <summary>
    /// Phase 15: ê°•í™”ëœ í—¤ë” ê³„ì¸µ êµ¬ì¡° ì¼ê´€ì„± ê³„ì‚°
    /// ë” ì •êµí•œ ê³„ì¸µ êµ¬ì¡° ë¶„ì„ ë° ë²ˆí˜¸ ì²´ê³„ ì¸ì‹ í¬í•¨
    /// </summary>
    private static double CalculateHeaderConsistency(string content)
    {
        var markdownHeaderMatches = Regex.Matches(content, @"^#{1,6}\s+", RegexOptions.Multiline);
        var numberedHeaderMatches = Regex.Matches(content, @"^\d+[\.\)]\s+\w+", RegexOptions.Multiline);
        var hierarchicalHeaderMatches = Regex.Matches(content, @"^\d+\.\d+[\.\)]*\s+\w+", RegexOptions.Multiline);

        var totalHeaders = markdownHeaderMatches.Count + numberedHeaderMatches.Count + hierarchicalHeaderMatches.Count;
        if (totalHeaders <= 1) return 1.0;

        var consistency = 0.0;
        var weights = new List<double>();

        // 1. ë§ˆí¬ë‹¤ìš´ í—¤ë” ì¼ê´€ì„± ë¶„ì„
        if (markdownHeaderMatches.Count > 1)
        {
            var mdLevels = markdownHeaderMatches.Cast<Match>()
                .Select(m => m.Value.Count(c => c == '#'))
                .ToList();

            var mdConsistency = CalculateMarkdownLevelConsistency(mdLevels);
            consistency += mdConsistency * markdownHeaderMatches.Count;
            weights.Add(markdownHeaderMatches.Count);
        }

        // 2. ë²ˆí˜¸ ì²´ê³„ í—¤ë” ì¼ê´€ì„± ë¶„ì„
        if (numberedHeaderMatches.Count > 1)
        {
            var numberedConsistency = AnalyzeNumberedSequence(numberedHeaderMatches);
            consistency += numberedConsistency * numberedHeaderMatches.Count;
            weights.Add(numberedHeaderMatches.Count);
        }

        // 3. ê³„ì¸µì  ë²ˆí˜¸ ì²´ê³„ ì¼ê´€ì„± ë¶„ì„ (1.1, 1.2, 2.1 ë“±)
        if (hierarchicalHeaderMatches.Count > 1)
        {
            var hierarchicalConsistency = AnalyzeHierarchicalSequence(hierarchicalHeaderMatches);
            consistency += hierarchicalConsistency * hierarchicalHeaderMatches.Count;
            weights.Add(hierarchicalHeaderMatches.Count);
        }

        // 4. í˜¼í•© ì²´ê³„ íŒ¨ë„í‹° (ì„œë¡œ ë‹¤ë¥¸ ë²ˆí˜¸ ì²´ê³„ê°€ ì„ì—¬ìˆìœ¼ë©´ ì¼ê´€ì„± ê°ì†Œ)
        var mixedSystemsPenalty = CalculateMixedSystemsPenalty(
            markdownHeaderMatches.Count,
            numberedHeaderMatches.Count,
            hierarchicalHeaderMatches.Count);

        var weightedConsistency = weights.Count > 0 ? consistency / weights.Sum() : 1.0;
        return Math.Max(0.0, weightedConsistency - mixedSystemsPenalty);
    }

    /// <summary>
    /// ë§ˆí¬ë‹¤ìš´ í—¤ë” ë ˆë²¨ ì¼ê´€ì„± ê³„ì‚°
    /// </summary>
    private static double CalculateMarkdownLevelConsistency(List<int> levels)
    {
        var levelJumps = levels.Zip(levels.Skip(1), (a, b) => Math.Abs(a - b)).ToList();
        var avgJump = levelJumps.Count != 0 ? levelJumps.Average() : 0;

        // ë ˆë²¨ ì í”„ê°€ 1 ì´í•˜ë©´ ì¢‹ì€ ì¼ê´€ì„±, 2 ì´ìƒì´ë©´ ì¼ê´€ì„± ì €í•˜
        return Math.Max(0.0, 1.0 - (avgJump - 1.0) * 0.3);
    }

    /// <summary>
    /// ë²ˆí˜¸ ì²´ê³„ ìˆœì„œ ì¼ê´€ì„± ë¶„ì„
    /// </summary>
    private static double AnalyzeNumberedSequence(MatchCollection matches)
    {
        var numbers = new List<int>();
        foreach (Match match in matches)
        {
            var numberText = Regex.Match(match.Value, @"^\d+").Value;
            if (int.TryParse(numberText, out var number))
            {
                numbers.Add(number);
            }
        }

        if (numbers.Count <= 1) return 1.0;

        var isSequential = true;
        var expectedNext = numbers[0] + 1;

        for (int i = 1; i < numbers.Count; i++)
        {
            if (numbers[i] != expectedNext)
            {
                isSequential = false;
                break;
            }
            expectedNext = numbers[i] + 1;
        }

        // ìˆœì°¨ì ì´ë©´ 1.0, ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ë¶€ë¶„ ì ìˆ˜
        return isSequential ? 1.0 : 0.6;
    }

    /// <summary>
    /// ê³„ì¸µì  ë²ˆí˜¸ ì²´ê³„ ì¼ê´€ì„± ë¶„ì„ (1.1, 1.2, 2.1 í˜•íƒœ)
    /// </summary>
    private static double AnalyzeHierarchicalSequence(MatchCollection matches)
    {
        var hierarchicalNumbers = new List<(int major, int minor)>();

        foreach (Match match in matches)
        {
            var parts = match.Value.Split('.');
            if (parts.Length >= 2 &&
                int.TryParse(parts[0], out var major) &&
                int.TryParse(parts[1].TrimEnd('.', ')', ' ').Split(' ')[0], out var minor))
            {
                hierarchicalNumbers.Add((major, minor));
            }
        }

        if (hierarchicalNumbers.Count <= 1) return 1.0;

        var consistency = 0.0;
        var groupedByMajor = hierarchicalNumbers.GroupBy(h => h.major).ToList();

        foreach (var group in groupedByMajor)
        {
            var minors = group.Select(g => g.minor).OrderBy(m => m).ToList();

            // ê° ì£¼ìš” ë²ˆí˜¸ ê·¸ë£¹ ë‚´ì—ì„œ ë¶€ë²ˆí˜¸ê°€ ìˆœì°¨ì ì¸ì§€ í™•ì¸
            var isSequential = true;
            for (int i = 1; i < minors.Count; i++)
            {
                if (minors[i] != minors[i - 1] + 1)
                {
                    isSequential = false;
                    break;
                }
            }

            consistency += isSequential ? 1.0 : 0.7;
        }

        return consistency / groupedByMajor.Count;
    }

    /// <summary>
    /// í˜¼í•© í—¤ë” ì²´ê³„ íŒ¨ë„í‹° ê³„ì‚°
    /// </summary>
    private static double CalculateMixedSystemsPenalty(int markdownCount, int numberedCount, int hierarchicalCount)
    {
        var systemsUsed = 0;
        if (markdownCount > 0) systemsUsed++;
        if (numberedCount > 0) systemsUsed++;
        if (hierarchicalCount > 0) systemsUsed++;

        // í•˜ë‚˜ì˜ ì²´ê³„ë§Œ ì‚¬ìš©í•˜ë©´ íŒ¨ë„í‹° ì—†ìŒ, ì—¬ëŸ¬ ì²´ê³„ í˜¼ìš© ì‹œ íŒ¨ë„í‹°
        return systemsUsed switch
        {
            1 => 0.0,  // ë‹¨ì¼ ì²´ê³„ - íŒ¨ë„í‹° ì—†ìŒ
            2 => 0.1,  // 2ê°œ ì²´ê³„ í˜¼ìš© - ì•½ê°„ì˜ íŒ¨ë„í‹°
            3 => 0.2,  // 3ê°œ ì²´ê³„ í˜¼ìš© - ë” í° íŒ¨ë„í‹°
            _ => 0.0
        };
    }

    private static double CalculateListConsistency(string content)
    {
        var lines = content.Split('\n');
        var listLines = lines.Where(line => 
            Regex.IsMatch(line.Trim(), @"^[-â€¢*]\s+") ||
            Regex.IsMatch(line.Trim(), @"^\d+\.\s+")
        ).ToList();

        if (listLines.Count <= 1) return 1.0;

        // ë¦¬ìŠ¤íŠ¸ ë§ˆì»¤ì˜ ì¼ê´€ì„± í™•ì¸
        var hasUnorderedMarkers = listLines.Any(line => Regex.IsMatch(line.Trim(), @"^[-â€¢*]\s+"));
        var hasOrderedMarkers = listLines.Any(line => Regex.IsMatch(line.Trim(), @"^\d+\.\s+"));

        // í˜¼ì¬í•˜ì§€ ì•Šìœ¼ë©´ ì¼ê´€ì„± ë†’ìŒ
        return (hasUnorderedMarkers && hasOrderedMarkers) ? 0.7 : 1.0;
    }

    private static double CalculateTableConsistency(string content)
    {
        var tableLines = content.Split('\n').Where(line => line.Contains('|')).ToList();
        if (tableLines.Count <= 1) return 1.0;

        // í…Œì´ë¸” í–‰ì˜ ì»¬ëŸ¼ ìˆ˜ ì¼ê´€ì„± í™•ì¸
        var columnCounts = tableLines.Select(line => line.Split('|', StringSplitOptions.RemoveEmptyEntries).Length).ToList();
        var avgColumns = columnCounts.Average();
        var maxDeviation = columnCounts.Max(c => Math.Abs(c - avgColumns));

        return Math.Max(0.0, 1.0 - (maxDeviation / avgColumns));
    }

    /// <summary>
    /// Phase 11: Apply vector search optimization to chunk
    /// </summary>
    private static void ApplyVectorSearchOptimization(DocumentChunk chunk, ChunkingOptions options)
    {
        try
        {
            // 1. Vector search optimization
            var vectorOptions = new VectorSearchOptions
            {
                MinSemanticDensity = 0.3,
                MaxSemanticDensity = 0.8,
                LowercaseNormalization = false, // Preserve case for technical terms
                ReplaceUrls = true,
                NormalizeNumbers = false, // Keep exact numbers for technical content
                RemoveSpecialCharacters = false,
                MaxTokensPerChunk = options.MaxChunkSize / 4 // Estimate tokens
            };

            var optimizedChunk = _vectorSearchOptimizer.OptimizeForVectorSearch(chunk, vectorOptions);

            // Store optimization results in chunk properties
            chunk.Properties["VectorOptimized"] = true;
            chunk.Properties["SemanticDensity"] = optimizedChunk.SemanticDensity;
            chunk.Properties["OptimizationQuality"] = optimizedChunk.OptimizationMetrics.QualityScore;
            chunk.Properties["EmbeddingHints"] = optimizedChunk.EmbeddingHints;

            // 2. Search metadata enrichment
            var enrichmentOptions = new EnrichmentOptions
            {
                MaxKeywordsPerChunk = 15,
                MaxPhrasesPerChunk = 8,
                EnableSemanticTagging = true,
                EnableRelationshipMapping = false // Skip for performance
            };

            var enrichedChunk = _metadataEnricher.EnrichWithSearchMetadata(chunk, enrichmentOptions);

            // Store enrichment results
            chunk.Properties["SearchKeywords"] = enrichedChunk.ExtractedKeywords.TfIdfKeywords.Take(10).Select(k => k.Keyword).ToList();
            chunk.Properties["SemanticTags"] = enrichedChunk.SemanticTags.Topics;
            chunk.Properties["ContentType"] = enrichedChunk.SemanticTags.ContentType;
            chunk.Properties["SearchScores"] = enrichedChunk.SearchScores;

            // 3. Hybrid search preprocessing
            var hybridOptions = new HybridSearchOptions
            {
                BM25Options = new BM25Options
                {
                    RemoveStopWords = true,
                    ApplyStemming = false, // Keep original terms
                    MinTokenLength = 2,
                    K1 = 1.2,
                    B = 0.75
                },
                EnableSynonymMapping = false, // Skip for performance
                EnableQueryExpansion = true,
                DefaultKeywordWeight = 0.6,
                DefaultSemanticWeight = 0.4
            };

            var hybridResult = _hybridPreprocessor.PreprocessForHybridSearch(chunk, hybridOptions);

            // Store hybrid preprocessing results
            chunk.Properties["HybridPreprocessed"] = true;
            chunk.Properties["BM25Terms"] = hybridResult.BM25Preprocessing.TermFrequencies.Keys.Take(10).ToList();
            chunk.Properties["HybridRatio"] = hybridResult.WeightCalculationInfo.RecommendedHybridRatio;
            chunk.Properties["RerankingHints"] = hybridResult.RerankingHints;

            // 4. Search quality evaluation
            var qualityOptions = new SearchQualityOptions
            {
                MinQualityThreshold = 0.6,
                EnableABTesting = false, // Skip for performance
                EvaluationSampleSize = 100,
                EvaluationPeriod = TimeSpan.FromDays(1)
            };

            var qualityResult = _qualityEvaluator.EvaluateSearchQuality(chunk, qualityOptions);

            // Store quality evaluation results
            chunk.Properties["SearchQualityScore"] = qualityResult.OverallQualityScore;
            chunk.Properties["RetrievalRecall"] = qualityResult.RetrievalRecall?.PredictedRecallScore ?? 0.5;
            chunk.Properties["DistinctivenessScore"] = qualityResult.DistinctivenessScore?.OverallDistinctiveness ?? 0.5;
            chunk.Properties["SemanticCompleteness"] = qualityResult.SemanticCompleteness?.SelfContainment ?? 0.5;

            // 5. Update overall quality score with search optimization
            var originalQuality = chunk.QualityScore;
            var searchQuality = qualityResult.OverallQualityScore;
            
            // Weighted combination: 70% original quality + 30% search quality
            chunk.QualityScore = (originalQuality * 0.7) + (searchQuality * 0.3);

            // Mark as search-optimized
            chunk.Properties["SearchOptimized"] = true;
            chunk.Properties["OptimizationTimestamp"] = DateTime.UtcNow;
        }
        catch (Exception ex)
        {
            // Log error but don't fail chunk creation
            chunk.Properties["SearchOptimizationError"] = ex.Message;
            chunk.Properties["SearchOptimized"] = false;
        }
    }

    /// <summary>
    /// Phase 12: Apply graph search optimization to chunk
    /// </summary>
    private static void ApplyGraphSearchOptimization(DocumentChunk chunk, ChunkingOptions options)
    {
        try
        {
            // 1. Entity extraction
            var entityOptions = new EntityExtractionOptions
            {
                EnableNER = true,
                EnableRelationshipExtraction = true,
                EnableCoreferenceResolution = true,
                MaxEntitiesPerChunk = 50,
                MinEntityConfidence = 0.5,
                ExtractPersons = true,
                ExtractOrganizations = true,
                ExtractLocations = true,
                ExtractDates = true,
                ExtractConcepts = true
            };

            var entityResult = _entityExtractor.ExtractEntitiesAndRelationships(chunk, entityOptions);

            // Store entity extraction results
            chunk.Properties["EntitiesExtracted"] = entityResult.NamedEntities.Count;
            chunk.Properties["RelationshipsExtracted"] = entityResult.ExtractedRelationships.Count;
            chunk.Properties["CoreferenceChains"] = entityResult.CoreferenceChains.Count;
            chunk.Properties["EntityTypes"] = entityResult.NamedEntities.Select(e => e.Type).Distinct().ToList();

            // 2. Graph structure generation
            var graphOptions = new GraphGenerationOptions
            {
                EnableRDFGeneration = true,
                EnableHierarchicalStructures = true,
                EnableTemporalAnalysis = true,
                EnableSpatialAnalysis = true,
                EnableCausalAnalysis = false, // Skip for performance
                MinConfidenceThreshold = 0.6,
                MaxTriplesPerChunk = 100
            };

            var graphResult = _graphGenerator.GenerateGraphStructure(entityResult, graphOptions);

            // Store graph structure results
            chunk.Properties["GraphTriples"] = graphResult.Triples.Count;
            chunk.Properties["HierarchicalStructures"] = graphResult.HierarchicalStructures.Count;
            chunk.Properties["TemporalRelationships"] = graphResult.TemporalRelationships.Count;
            chunk.Properties["SpatialRelationships"] = graphResult.SpatialRelationships.Count;
            chunk.Properties["GraphMetrics"] = graphResult.GraphMetrics;

            // 3. Ontology mapping
            var ontologyOptions = new OntologyMappingOptions
            {
                MinDomainConfidence = 0.7,
                EnableSchemaInference = true,
                ApplyPropertyMappings = true,
                ValidateConsistency = true
            };

            var ontologyResult = _ontologyMapper.MapToOntology(graphResult, ontologyOptions);

            // Store ontology mapping results
            chunk.Properties["OntologyDomain"] = ontologyResult.DomainOntology.Domain;
            chunk.Properties["SchemaEntityTypes"] = ontologyResult.InferredSchema.EntityTypes.Count;
            chunk.Properties["MappedTriples"] = ontologyResult.MappedTriples.Count;
            chunk.Properties["TypedEntities"] = ontologyResult.TypedEntities.Count;
            chunk.Properties["OntologyQuality"] = ontologyResult.QualityMetrics;

            // 4. Graph quality assurance
            var qualityOptions = new GraphQualityOptions
            {
                AutoFix = false, // Skip auto-fix for performance
                MinConnectionThreshold = 2,
                ValidateSemanticConsistency = true,
                DetectCycles = true,
                IdentifyOrphans = true
            };

            var qualityResult = _graphQualityAssurance.AssessGraphQuality(ontologyResult, qualityOptions);

            // Store quality assessment results
            chunk.Properties["GraphQualityGrade"] = qualityResult.QualityScores.QualityGrade;
            chunk.Properties["ConsistencyScore"] = qualityResult.ConsistencyReport.ConsistencyScore;
            chunk.Properties["CompletenessScore"] = qualityResult.CompletenessReport.OverallCompletenessScore;
            chunk.Properties["StructuralIntegrityScore"] = qualityResult.StructuralIntegrityReport.IntegrityScore;
            chunk.Properties["HasCycles"] = qualityResult.CyclicReferenceReport.HasCycles;
            chunk.Properties["OrphanNodesCount"] = qualityResult.OrphanNodeReport.OrphanCount;

            // 5. Generate improvement recommendations
            if (qualityResult.ImprovementRecommendations.Any())
            {
                chunk.Properties["GraphImprovementRecommendations"] = qualityResult.ImprovementRecommendations
                    .Take(3) // Top 3 recommendations
                    .Select(r => new { r.Priority, r.Category, r.Description })
                    .ToList();
            }

            // 6. Update chunk properties with graph-specific metadata
            if (entityResult.NamedEntities.Any())
            {
                // Extract key entities for search optimization
                var keyEntities = entityResult.NamedEntities
                    .Where(e => e.Confidence > 0.7)
                    .OrderByDescending(e => e.Confidence)
                    .Take(10)
                    .Select(e => e.Value)
                    .ToList();

                chunk.Properties["GraphKeyEntities"] = keyEntities;
                
                // Add entity-based keywords to existing search keywords
                if (chunk.Properties.ContainsKey("SearchKeywords"))
                {
                    var existingKeywords = (List<string>)chunk.Properties["SearchKeywords"];
                    var combinedKeywords = existingKeywords.Union(keyEntities).Take(15).ToList();
                    chunk.Properties["SearchKeywords"] = combinedKeywords;
                }
            }

            // 7. Enhance content type classification
            if (ontologyResult.DomainOntology.Domain != "general")
            {
                chunk.Properties["GraphDomain"] = ontologyResult.DomainOntology.Domain;
                
                // Update document domain if graph provides more specific classification
                if (chunk.DocumentDomain == "General" || string.IsNullOrEmpty(chunk.DocumentDomain))
                {
                    chunk.DocumentDomain = ontologyResult.DomainOntology.Domain.Substring(0, 1).ToUpper() + 
                                         ontologyResult.DomainOntology.Domain.Substring(1).ToLower();
                }
            }

            // 8. Calculate graph-enhanced relevance score
            var graphRelevance = CalculateGraphRelevanceScore(entityResult, graphResult, qualityResult);
            
            // Update overall relevance score: 70% original + 30% graph-based
            var originalRelevance = chunk.RelevanceScore;
            chunk.RelevanceScore = (originalRelevance * 0.7) + (graphRelevance * 0.3);

            // 9. Update overall quality score with graph quality
            var originalQuality = chunk.QualityScore;
            var graphQuality = qualityResult.QualityScores.OverallQualityScore;
            
            // Weighted combination: 80% current quality + 20% graph quality
            chunk.QualityScore = (originalQuality * 0.8) + (graphQuality * 0.2);

            // Mark as graph-optimized
            chunk.Properties["GraphOptimized"] = true;
            chunk.Properties["GraphOptimizationTimestamp"] = DateTime.UtcNow;
        }
        catch (Exception ex)
        {
            // Log error but don't fail chunk creation
            chunk.Properties["GraphOptimizationError"] = ex.Message;
            chunk.Properties["GraphOptimized"] = false;
        }
    }

    /// <summary>
    /// Calculate relevance score based on graph structure quality
    /// </summary>
    private static double CalculateGraphRelevanceScore(EntityExtractionResult entityResult, 
        GraphStructureResult graphResult, GraphQualityResult qualityResult)
    {
        var scores = new List<double>();

        // Entity richness score (0.0-1.0)
        var entityScore = Math.Min(1.0, entityResult.NamedEntities.Count / 10.0); // 10+ entities = max score
        scores.Add(entityScore);

        // Relationship density score (0.0-1.0)
        var relationshipScore = entityResult.NamedEntities.Count > 0 
            ? Math.Min(1.0, entityResult.ExtractedRelationships.Count / (double)entityResult.NamedEntities.Count)
            : 0.0;
        scores.Add(relationshipScore);

        // Graph connectivity score (0.0-1.0)
        var connectivityScore = graphResult.Triples.Count > 0 
            ? Math.Min(1.0, graphResult.Triples.Count / 20.0) // 20+ triples = max score
            : 0.0;
        scores.Add(connectivityScore);

        // Quality score (0.0-1.0)
        var qualityScore = qualityResult.QualityScores.OverallQualityScore;
        scores.Add(qualityScore);

        return scores.Average();
    }

    [GeneratedRegex(@"[.!?]+\s+", RegexOptions.Compiled)]
    private static partial Regex MyRegex();
}